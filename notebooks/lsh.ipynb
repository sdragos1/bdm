{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b92254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import hashlib\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import xml.etree.ElementTree as Element\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Iterator\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, Row\n",
    "\n",
    "HDFS_NAMENODE: str = \"hdfs://192.168.0.12:9000\"\n",
    "DATA_DIR: str = f\"{HDFS_NAMENODE}/input/similar_documents\"\n",
    "OUTPUT_DIR: str = f\"{HDFS_NAMENODE}/output/similar_documents\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EffectivenessMetrics:\n",
    "    true_positives: int\n",
    "    false_positives: int\n",
    "    false_negatives: int\n",
    "    true_negatives: int\n",
    "    accuracy: float\n",
    "    precision: float\n",
    "    recall: float\n",
    "    f1_score: float\n",
    "    documents_with_no_topics: int = 0\n",
    "\n",
    "    def print_metrics(self):\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"EFFECTIVENESS METRICS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"True Positives (TP):   {self.true_positives:,}\")\n",
    "        print(f\"False Positives (FP):  {self.false_positives:,}\")\n",
    "        print(f\"False Negatives (FN):  {self.false_negatives:,}\")\n",
    "        print(f\"True Negatives (TN):   {self.true_negatives:,}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Accuracy:              {self.accuracy:.4f} ({self.accuracy * 100:.2f}%)\")\n",
    "        print(f\"Precision:             {self.precision:.4f} ({self.precision * 100:.2f}%)\")\n",
    "        print(f\"Recall (Sensitivity):  {self.recall:.4f} ({self.recall * 100:.2f}%)\")\n",
    "        print(f\"F1 Score:              {self.f1_score:.4f}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Document with no topics:   {self.documents_with_no_topics:,}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    newid: str\n",
    "    body: str\n",
    "    topics: List[str]\n",
    "\n",
    "\n",
    "def parse_reuters_xml(xml_content: str, test_mode: bool) -> List[Document]:\n",
    "    documents: List[Document] = []\n",
    "    reuters_pattern: str = r'<REUTERS[^>]*>.*?</REUTERS>'\n",
    "    doc_matches: List[str] = re.findall(reuters_pattern, xml_content, re.DOTALL)\n",
    "\n",
    "    for document in doc_matches:\n",
    "        try:\n",
    "            document = re.sub(r'&#\\d+;', ' ', document)\n",
    "            root = Element.fromstring(document)\n",
    "\n",
    "            test_split = root.get('LEWISSPLIT', '') == 'TEST'\n",
    "            if test_mode and not test_split:\n",
    "                continue\n",
    "\n",
    "            newid: str = root.get('NEWID', '')\n",
    "\n",
    "            body: str = ''\n",
    "\n",
    "            text_elem = root.find('TEXT')\n",
    "            if text_elem is not None:\n",
    "                body_elem = text_elem.find('BODY')\n",
    "                if body_elem is not None and body_elem.text:\n",
    "                    body = body_elem.text.strip()\n",
    "\n",
    "            topics: list[str] = [d.text for d in root.findall('./TOPICS/D') if d.text]\n",
    "\n",
    "            doc = Document(newid=newid, body=body, topics=topics)\n",
    "            documents.append(doc)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "def generate_k_shingles(text: str, k: int) -> List[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r'\\breuter\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\d{1,2}-[A-Z]{3}-\\d{2,4}', '', text)\n",
    "\n",
    "    stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for'}\n",
    "\n",
    "    text = re.sub(r\"[^a-z0-9]+\", \" \", text)\n",
    "    tokens = [t for t in text.split() if t and t not in stopwords]\n",
    "\n",
    "    if len(tokens) < k:\n",
    "        return []\n",
    "\n",
    "    shingles = []\n",
    "    for i in range(len(tokens) - k + 1):\n",
    "        shingle = \" \".join(tokens[i:i + k])\n",
    "        shingles.append(shingle)\n",
    "\n",
    "    return list(set(shingles))\n",
    "\n",
    "\n",
    "def compute_idf(all_documents: List[Document], k: int) -> Dict[str, float]:\n",
    "    \"\"\"Compute IDF for all shingles\"\"\"\n",
    "    doc_count = len(all_documents)\n",
    "    shingle_doc_freq = Counter()\n",
    "\n",
    "    for doc in all_documents:\n",
    "        if doc.body:\n",
    "            shingles = set(generate_k_shingles(doc.body, k))\n",
    "            for shingle in shingles:\n",
    "                shingle_doc_freq[shingle] += 1\n",
    "\n",
    "    idf = {}\n",
    "    for shingle, freq in shingle_doc_freq.items():\n",
    "        idf[shingle] = math.log(doc_count / freq)\n",
    "\n",
    "    return idf\n",
    "\n",
    "\n",
    "def generate_minhash_signature_weighted(shingles_list: List[str],\n",
    "                                        num_hashes: int,\n",
    "                                        hash_params: List[Tuple[int, int]],\n",
    "                                        idf_scores: Dict[str, float]) -> List[int]:\n",
    "    if not shingles_list:\n",
    "        return [2147483647] * num_hashes\n",
    "\n",
    "    prime = 2147483647\n",
    "    signature = [prime] * num_hashes\n",
    "\n",
    "    weighted_shingles = []\n",
    "    for shingle in shingles_list:\n",
    "        weight = int(idf_scores.get(shingle, 1.0) * 3)\n",
    "        weighted_shingles.extend([shingle] * max(1, weight))\n",
    "\n",
    "    shingle_hashes = [int(hashlib.md5(shg.encode()).hexdigest(), 16) % prime\n",
    "                      for shg in weighted_shingles]\n",
    "\n",
    "    for i, (a, b) in enumerate(hash_params):\n",
    "        min_hash = min((a * sh + b) % prime for sh in shingle_hashes)\n",
    "        signature[i] = int(min_hash)\n",
    "\n",
    "    return signature\n",
    "\n",
    "\n",
    "def generate_lsh_bands(signature: List[int], bands_len: int, rows_per_band: int,\n",
    "                       band_hash_params: List[List[Tuple[int, int]]]) -> List[Tuple[int, str]]:\n",
    "    bands: List[Tuple[int, str]] = []\n",
    "    prime: int = 2147483647\n",
    "\n",
    "    for band_idx in range(bands_len):\n",
    "        start_idx: int = band_idx * rows_per_band\n",
    "        end_idx: int = start_idx + rows_per_band\n",
    "        band_values: Tuple[int, ...] = tuple(signature[start_idx:end_idx])\n",
    "\n",
    "        band_str: str = str(band_values)\n",
    "        band_int_hash: int = int(hashlib.md5(band_str.encode()).hexdigest(), 16) % prime\n",
    "\n",
    "        band_hashes: List[int] = [\n",
    "            (a * band_int_hash + b) % prime\n",
    "            for a, b in band_hash_params[band_idx]\n",
    "        ]\n",
    "\n",
    "        combined_hash: str = hashlib.md5(str(tuple(band_hashes)).encode()).hexdigest()\n",
    "        bands.append((band_idx, combined_hash))\n",
    "\n",
    "    return bands\n",
    "\n",
    "\n",
    "def process_partition(documents_iter: Iterator[Document], k: int, num_hashes: int,\n",
    "                      bands_len: int, rows_per_band: int, hash_params: List[Tuple[int, int]],\n",
    "                      band_hash_params: List[List[Tuple[int, int]]], idf_scores: Dict[str, float]) -> Iterator[\n",
    "    Tuple[str, str, List[str], List[int], List[Tuple[int, str]]]]:\n",
    "    for document in documents_iter:\n",
    "        if document.body:\n",
    "            shingles_list: List[str] = generate_k_shingles(document.body, k)\n",
    "            if shingles_list:\n",
    "                signature: List[int] = generate_minhash_signature_weighted(shingles_list, num_hashes, hash_params,\n",
    "                                                                           idf_scores)\n",
    "                bands: List[Tuple[int, str]] = generate_lsh_bands(signature, bands_len, rows_per_band, band_hash_params)\n",
    "                yield document.newid, document.body, shingles_list, signature, bands\n",
    "\n",
    "\n",
    "def compute_effectiveness_metrics(candidate_pairs_df: DataFrame, all_docs: List[Document]) -> EffectivenessMetrics:\n",
    "    doc_topics: Dict[str, set] = {doc.newid: set(doc.topics) for doc in all_docs if doc.topics}\n",
    "    doc_ids = list(doc_topics.keys())\n",
    "\n",
    "    candidate_set = set()\n",
    "    for row in candidate_pairs_df.collect():\n",
    "        candidate_set.add((row.doc_id_1, row.doc_id_2))\n",
    "        candidate_set.add((row.doc_id_2, row.doc_id_1))\n",
    "\n",
    "    tp = fp = fn = tn = 0\n",
    "\n",
    "    for i in range(len(doc_ids)):\n",
    "        for j in range(i + 1, len(doc_ids)):\n",
    "            d1, d2 = doc_ids[i], doc_ids[j]\n",
    "            actual_similar = len(doc_topics[d1].intersection(doc_topics[d2])) > 0\n",
    "            predicted_similar = (d1, d2) in candidate_set\n",
    "\n",
    "            if predicted_similar and actual_similar:\n",
    "                tp += 1\n",
    "            elif predicted_similar and not actual_similar:\n",
    "                fp += 1\n",
    "            elif not predicted_similar and actual_similar:\n",
    "                fn += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return EffectivenessMetrics(\n",
    "        true_positives=tp,\n",
    "        false_positives=fp,\n",
    "        false_negatives=fn,\n",
    "        true_negatives=tn,\n",
    "        accuracy=accuracy,\n",
    "        precision=precision,\n",
    "        recall=recall,\n",
    "        f1_score=f1_score,\n",
    "        documents_with_no_topics=len(all_docs) - len(doc_topics)\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='LSH Similar Documents Pipeline')\n",
    "    parser.add_argument('--k', type=int, default=3,\n",
    "                        help='k-shingle size (default: 3)')\n",
    "    parser.add_argument('--H', type=int, default=100,\n",
    "                        help='Number of MinHash functions (default: 100)')\n",
    "    parser.add_argument('--b', type=int, default=50,\n",
    "                        help='Number of bands (default: 50)')\n",
    "    parser.add_argument('--r', type=int, default=None,\n",
    "                        help='Number of rows per band (default: H/b)')\n",
    "    parser.add_argument('--band-hashes', type=int, default=2,\n",
    "                        help='Number of hash functions for hashing each band (default: 2)')\n",
    "    parser.add_argument('--min-bands', type=int, default=1,\n",
    "                        help='Minimum number of band matches to consider pair as candidate (default: 1)')\n",
    "    parser.add_argument('--test', action='store_true', default=False,\n",
    "                        help='Run the pipeline on the test split (default: False)')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    K_SHINGLE_SIZE = args.k\n",
    "    NUM_HASHES = args.H\n",
    "    NUM_BANDS = args.b\n",
    "    MIN_BAND_MATCHES = args.min_bands\n",
    "\n",
    "    if args.r is None:\n",
    "        ROWS_PER_BAND = NUM_HASHES // NUM_BANDS\n",
    "        if NUM_HASHES % NUM_BANDS != 0:\n",
    "            print(f\"Warning: H ({NUM_HASHES}) not divisible by b ({NUM_BANDS}), using r={ROWS_PER_BAND}\")\n",
    "    else:\n",
    "        ROWS_PER_BAND = args.r\n",
    "        NUM_BANDS = NUM_HASHES // ROWS_PER_BAND\n",
    "        if NUM_HASHES % ROWS_PER_BAND != 0:\n",
    "            raise ValueError(\"H must be multiple of r\")\n",
    "\n",
    "    NUM_BAND_HASHES = args.band_hashes\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"LSH Similar Documents Pipeline - Configuration\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"k (k-shingle size): {K_SHINGLE_SIZE}\")\n",
    "    print(f\"H (number of MinHash functions): {NUM_HASHES}\")\n",
    "    print(f\"b (number of bands): {NUM_BANDS}\")\n",
    "    print(f\"r (rows per band): {ROWS_PER_BAND}\")\n",
    "    print(f\"Number of hash functions per band: {NUM_BAND_HASHES}\")\n",
    "    print(f\"Minimum band matches for candidates: {MIN_BAND_MATCHES}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Reuters Similar Documents\") \\\n",
    "        .master(\"spark://192.168.0.2:7077\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "        .config(\"spark.rpc.message.maxSize\", \"256\") \\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"512m\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    sc = spark.sparkContext\n",
    "    sc.setLogLevel(\"WARN\")\n",
    "\n",
    "    prime = 2147483647\n",
    "    random.seed(42)\n",
    "    hash_params = [(random.randint(1, prime - 1), random.randint(0, prime - 1)) for _ in range(NUM_HASHES)]\n",
    "\n",
    "    band_hash_params = []\n",
    "    for band_idx in range(NUM_BANDS):\n",
    "        random.seed(band_idx * 1000)\n",
    "        band_hash_params.append(\n",
    "            [(random.randint(1, prime - 1), random.randint(0, prime - 1)) for _ in range(NUM_BAND_HASHES)])\n",
    "\n",
    "    print(\"\\nLoading Reuters files from HDFS...\")\n",
    "\n",
    "    file_paths = [f\"{DATA_DIR}/reut2-{i:03d}.sgm\" for i in range(22)]\n",
    "\n",
    "    all_documents: List[Document] = []\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            file_rdd = sc.textFile(file_path)\n",
    "            content = \"\\n\".join(file_rdd.collect())\n",
    "            docs = parse_reuters_xml(content, args.test)\n",
    "            all_documents.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "\n",
    "    if not all_documents:\n",
    "        print(\"No documents loaded. Exiting.\")\n",
    "        spark.stop()\n",
    "        exit(1)\n",
    "\n",
    "    hash_params_bc = sc.broadcast(hash_params)\n",
    "    band_hash_params_bc = sc.broadcast(band_hash_params)\n",
    "\n",
    "    docs_rdd = sc.parallelize(all_documents, numSlices=8)\n",
    "\n",
    "    print(\"\\nComputing IDF scores...\")\n",
    "    idf_scores = compute_idf(all_documents, K_SHINGLE_SIZE)\n",
    "    idf_scores_bc = sc.broadcast(idf_scores)\n",
    "\n",
    "\n",
    "    def process_with_params(docs_iter):\n",
    "        return process_partition(\n",
    "            docs_iter, K_SHINGLE_SIZE, NUM_HASHES, NUM_BANDS,\n",
    "            ROWS_PER_BAND, hash_params_bc.value, band_hash_params_bc.value, idf_scores_bc.value\n",
    "        )\n",
    "\n",
    "\n",
    "    processed_rdd = docs_rdd.mapPartitions(process_with_params).cache()\n",
    "\n",
    "    num_processed = processed_rdd.count()\n",
    "\n",
    "    if num_processed == 0:\n",
    "        print(\"No documents with shingles found. Exiting.\")\n",
    "        spark.stop()\n",
    "        exit(1)\n",
    "\n",
    "    print(f\"\\nProcessing {num_processed} documents... (only those with body and shingles)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Generating Set Representation (MxN matrix: Shingles x Documents)...\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    shingle_doc_rdd = processed_rdd.flatMap(\n",
    "        lambda x: [Row(shingle=shingle, doc_id=x[0], value='1') for shingle in x[2]]\n",
    "    )\n",
    "    set_rep_df = spark.createDataFrame(shingle_doc_rdd)\n",
    "\n",
    "    set_rep_path = f\"{OUTPUT_DIR}/set_representation\"\n",
    "    print(f\"Saving set representation to {set_rep_path}...\")\n",
    "    set_rep_df.repartition(4).write.mode(\"overwrite\").csv(set_rep_path, header=True, sep=\",\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Generating MinHash Signatures (HxN matrix: Hash Functions x Documents)...\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    signature_rdd = processed_rdd.flatMap(\n",
    "        lambda x: [Row(hash_function_index=str(i), doc_id=x[0], signature_value=str(sig_val))\n",
    "                   for i, sig_val in enumerate(x[3])]\n",
    "    )\n",
    "    sig_df = spark.createDataFrame(signature_rdd)\n",
    "\n",
    "    sig_path = f\"{OUTPUT_DIR}/minhash_signatures\"\n",
    "    sig_df.repartition(4).write.mode(\"overwrite\").csv(sig_path, header=True, sep=\",\")\n",
    "    print(f\"MinHash signatures saved: {sig_df.count()} entries\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Finding Candidate Pairs...\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    bands_rdd = processed_rdd.flatMap(\n",
    "        lambda x: [((band_id, band_hash), x[0]) for band_id, band_hash in x[4]]\n",
    "    )\n",
    "\n",
    "    buckets_rdd = bands_rdd.groupByKey().mapValues(list)\n",
    "\n",
    "\n",
    "    def generate_pairs(bucket_docs):\n",
    "        docs = list(bucket_docs)\n",
    "        if len(docs) < 2:\n",
    "            return []\n",
    "        pairs = []\n",
    "        for i in range(len(docs)):\n",
    "            for j in range(i + 1, len(docs)):\n",
    "                pair = tuple(sorted([docs[i], docs[j]]))\n",
    "                pairs.append((pair, 1))\n",
    "        return pairs\n",
    "\n",
    "\n",
    "    pair_counts_rdd = buckets_rdd.flatMap(lambda x: generate_pairs(x[1])) \\\n",
    "        .reduceByKey(lambda a, b: a + b) \\\n",
    "        .filter(lambda x: x[1] >= MIN_BAND_MATCHES)\n",
    "\n",
    "    candidate_pairs_rdd = pair_counts_rdd.map(\n",
    "        lambda x: Row(doc_id_1=x[0][0], doc_id_2=x[0][1], num_bands_matched=x[1])\n",
    "    )\n",
    "\n",
    "    candidate_df = spark.createDataFrame(candidate_pairs_rdd)\n",
    "\n",
    "    num_candidates = candidate_df.count()\n",
    "\n",
    "    if num_candidates == 0:\n",
    "        print(f\"No candidate pairs found (pairs matching in >= {MIN_BAND_MATCHES} bands).\")\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "        empty_schema = StructType([\n",
    "            StructField(\"doc_id_1\", StringType(), True),\n",
    "            StructField(\"doc_id_2\", StringType(), True),\n",
    "            StructField(\"num_bands_matched\", IntegerType(), True)\n",
    "        ])\n",
    "        candidate_df = spark.createDataFrame([], empty_schema)\n",
    "    else:\n",
    "        candidate_df = candidate_df.orderBy(col(\"num_bands_matched\").cast(IntegerType()).desc())\n",
    "\n",
    "        candidate_path = f\"{OUTPUT_DIR}/candidate_pairs\"\n",
    "        print(f\"Saving candidate pairs to {candidate_path}...\")\n",
    "        candidate_df.coalesce(1).write.mode(\"overwrite\").csv(candidate_path, header=True, sep=\",\")\n",
    "        print(f\"Candidate pairs saved: {num_candidates} pairs\")\n",
    "        candidate_df.show(20, truncate=False)\n",
    "\n",
    "    metrics = compute_effectiveness_metrics(candidate_df, all_documents)\n",
    "    metrics.print_metrics()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Pipeline completed!\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    hash_params_bc.unpersist()\n",
    "    band_hash_params_bc.unpersist()\n",
    "    idf_scores_bc.unpersist()\n",
    "    processed_rdd.unpersist()\n",
    "\n",
    "    spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
