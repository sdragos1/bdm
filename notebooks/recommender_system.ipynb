{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f06c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME: str = \"recommender_system\"\n",
    "HDFS_NAMENODE: str = \"hdfs://namenode:9000\"\n",
    "INPUT_DIR: str = f\"{HDFS_NAMENODE}/input/{PROJECT_NAME}\"\n",
    "OUTPUT_DIR: str = f\"{HDFS_NAMENODE}/output/{PROJECT_NAME}\"\n",
    "\n",
    "MASTER_URI = \"spark://spark-master:7077\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff04ffa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Spark 3.5.0\n",
      "+------+------+---+----------+--------+\n",
      "|UserID|Gender|Age|Occupation|Zip_code|\n",
      "+------+------+---+----------+--------+\n",
      "|     1|     F|  1|        10|   48067|\n",
      "|     2|     M| 56|        16|   70072|\n",
      "|     3|     M| 25|        15|   55117|\n",
      "|     4|     M| 45|         7|   02460|\n",
      "|     5|     M| 25|        20|   55455|\n",
      "|     6|     F| 50|         9|   55117|\n",
      "|     7|     M| 35|         1|   06810|\n",
      "|     8|     M| 25|        12|   11413|\n",
      "|     9|     M| 25|        17|   61614|\n",
      "|    10|     F| 35|         1|   95370|\n",
      "|    11|     F| 25|         1|   04093|\n",
      "|    12|     M| 25|        12|   32793|\n",
      "|    13|     M| 45|         1|   93304|\n",
      "|    14|     M| 35|         0|   60126|\n",
      "|    15|     M| 25|         7|   22903|\n",
      "|    16|     F| 35|         0|   20670|\n",
      "|    17|     M| 50|         1|   95350|\n",
      "|    18|     F| 18|         3|   95825|\n",
      "|    19|     M|  1|        10|   48073|\n",
      "|    20|     M| 25|        14|   55113|\n",
      "+------+------+---+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(PROJECT_NAME.capitalize) \\\n",
    "    .master(MASTER_URI) \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .config(\"spark.hadoop.dfs.client.use.datanode.hostname\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Connected to Spark {spark.version}\")\n",
    "\n",
    "\n",
    "userSchema = StructType([\n",
    "    StructField(\"UserID\",    IntegerType(), True),\n",
    "    StructField(\"Gender\",    StringType(),  True),\n",
    "    StructField(\"Age\",       IntegerType(), True),\n",
    "    StructField(\"Occupation\",StringType(),  True),\n",
    "    StructField(\"Zip_code\",  StringType(),  True)\n",
    "])\n",
    "\n",
    "df = spark.read.option(\"sep\", \"::\").csv(f\"{INPUT_DIR}/users.dat\", schema=userSchema)\n",
    "df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e17b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
