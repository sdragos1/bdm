{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3f06c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME: str = \"recommender_system\"\n",
    "HDFS_NAMENODE: str = \"hdfs://namenode:9000\"\n",
    "INPUT_DIR: str = f\"{HDFS_NAMENODE}/input/{PROJECT_NAME}\"\n",
    "OUTPUT_DIR: str = f\"{HDFS_NAMENODE}/output/{PROJECT_NAME}\"\n",
    "\n",
    "MASTER_URI = \"spark://spark-master:7077\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d491642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schemas\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    ")\n",
    "\n",
    "userSchema = StructType(\n",
    "    [\n",
    "        StructField(\"UserID\", IntegerType(), True),\n",
    "        StructField(\"Gender\", StringType(), True),\n",
    "        StructField(\"Age\", IntegerType(), True),\n",
    "        StructField(\"Occupation\", StringType(), True),\n",
    "        StructField(\"Zip_code\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "movieSchema = StructType(\n",
    "    [\n",
    "        StructField(\"MovieID\", IntegerType(), True),\n",
    "        StructField(\"Title\", StringType(), True),\n",
    "        StructField(\"Genres\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ratingSchema = StructType(\n",
    "    [\n",
    "        StructField(\"UserID\", IntegerType(), True),\n",
    "        StructField(\"MovieID\", IntegerType(), True),\n",
    "        StructField(\"Rating\", IntegerType(), True),\n",
    "        StructField(\"Timestamp\", StringType(), True),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13305105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def spark_session() -> SparkSession:\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(PROJECT_NAME.capitalize)\n",
    "        .master(MASTER_URI)\n",
    "        .config(\"spark.driver.memory\", \"4g\")\n",
    "        .config(\"spark.hadoop.fs.defaultFS\", HDFS_NAMENODE)\n",
    "        .config(\"spark.hadoop.dfs.client.use.datanode.hostname\", \"true\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    print(f\"Connected to Spark {spark.version}\")\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fb6f664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Spark 3.5.0\n",
      "Starting data conversion to Parquet...\n",
      "------------------------------\n",
      "Processing: hdfs://namenode:9000/input/recommender_system/ratings.dat\n",
      "Successfully converted to Parquet at: hdfs://namenode:9000/input/recommender_system/ratings_parquet\n",
      "------------------------------\n",
      "Processing: hdfs://namenode:9000/input/recommender_system/users.dat\n",
      "Successfully converted to Parquet at: hdfs://namenode:9000/input/recommender_system/users_parquet\n",
      "------------------------------\n",
      "Processing: hdfs://namenode:9000/input/recommender_system/movies.dat\n",
      "Successfully converted to Parquet at: hdfs://namenode:9000/input/recommender_system/movies_parquet\n",
      "------------------------------\n",
      "All files converted.\n",
      "CPU times: user 39.9 ms, sys: 18.6 ms, total: 58.5 ms\n",
      "Wall time: 9.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "spark = spark_session()\n",
    "\n",
    "\n",
    "def convert_dat_to_parquet(file_name: str, schema: StructType):\n",
    "    input_path = f\"{INPUT_DIR}/{file_name}.dat\"\n",
    "    output_path = f\"{INPUT_DIR}/{file_name}_parquet\"\n",
    "\n",
    "    print(f\"Processing: {input_path}\")\n",
    "\n",
    "    df = spark.read.option(\"sep\", \"::\").csv(input_path, schema=schema)\n",
    "    df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "    print(f\"Successfully converted to Parquet at: {output_path}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "print(\"Starting data conversion to Parquet...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "convert_dat_to_parquet(\"ratings\", ratingSchema)\n",
    "convert_dat_to_parquet(\"users\", userSchema)\n",
    "convert_dat_to_parquet(\"movies\", movieSchema)\n",
    "\n",
    "print(\"All files converted.\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "312db5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RATINGS_FILE = f\"{INPUT_DIR}/ratings_parquet\"\n",
    "USERS_FILE = f\"{INPUT_DIR}/users_parquet\"\n",
    "MOVIES_FILE = f\"{INPUT_DIR}/movies_parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafea65e",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "List the top-rated movies by all users. \n",
    "\n",
    "#### Output format\n",
    "A list of <movie, score> pairs\n",
    "sorted in descending order of ‘average’ rating score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff04ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "spark = spark_session()\n",
    "\n",
    "df = spark.read.parquet(RATINGS_FILE)\n",
    "\n",
    "agg_df = df.groupBy(\"MovieId\").agg(avg(\"Rating\").alias(\"AvgRating\"))\n",
    "\n",
    "sorted_df = agg_df.orderBy(col(\"AvgRating\").desc())\n",
    "\n",
    "sorted_df.cache()\n",
    "\n",
    "sorted_df = sorted_df.select(\n",
    "    col(\"MovieId\").alias(\"movie\"), col(\"AvgRating\").alias(\"score\")\n",
    ")\n",
    "print(f\"Writing results to '{OUTPUT_DIR}/average_movie_ratings'\\n\")\n",
    "sorted_df.coalesce(1).write.mode(\"overwrite\").csv(\n",
    "    f\"{OUTPUT_DIR}/average_movie_ratings\", header=True, sep=\",\"\n",
    ")\n",
    "\n",
    "sorted_df.unpersist()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c2ece8",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "List the top-rated movies grouped by gender, by age group, and by occupation, respectively. \n",
    "\n",
    "#### Output format\n",
    "3 sorted lists: <movie, gender, score> pairs, <movie, age group, score> pairs, <movie, occupation, score>\n",
    "sorted in descending order of ‘average’ rating score grouped by gender, by age group, and by occupation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c31e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.sql.functions import broadcast, col, avg\n",
    "\n",
    "spark = spark_session()\n",
    "\n",
    "ratings_df = spark.read.parquet(RATINGS_FILE)\n",
    "user_df = spark.read.parquet(USERS_FILE)\n",
    "\n",
    "ratings_users_df = ratings_df.join(broadcast(user_df), on=\"UserId\").select(\n",
    "    [\"MovieId\", \"Rating\", \"Gender\", \"Age\", \"Occupation\"]\n",
    ")\n",
    "\n",
    "\n",
    "def top_rated_movies_grouped_by(category: str):\n",
    "    print(f\"Listing top rated movies by {category}...\")\n",
    "    agg_df = ratings_users_df.groupBy(category, \"MovieID\").agg(\n",
    "        avg(col(\"Rating\")).alias(\"AvgRating\")\n",
    "    )\n",
    "    sorted_df = agg_df.orderBy(col(\"AvgRating\").desc())\n",
    "    sorted_df = sorted_df.select(\n",
    "        col(\"MovieID\").alias(\"movie\"),\n",
    "        col(category).alias(category.lower()),\n",
    "        col(\"AvgRating\").alias(\"score\"),\n",
    "    )\n",
    "\n",
    "    sorted_df.coalesce(1).write.mode(\"overwrite\").csv(\n",
    "        f\"{OUTPUT_DIR}/top_movies_by_{category.lower()}\", header=True, sep=\",\"\n",
    "    )\n",
    "\n",
    "\n",
    "top_rated_movies_grouped_by(\"Gender\")\n",
    "top_rated_movies_grouped_by(\"Age\")\n",
    "top_rated_movies_grouped_by(\"Occupation\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8763959",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "List the average rating score of each user for all movies, and grouped by genre, respectively. \n",
    "\n",
    "\n",
    "#### Output format\n",
    "two sorted lists: <user, score> pairs, <user, genre, score> pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e22f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "spark = spark_session()\n",
    "\n",
    "ratings_df = spark.read.parquet(RATINGS_FILE)\n",
    "movies_df = spark.read.parquet(MOVIES_FILE)\n",
    "\n",
    "user_agg = ratings_df.groupBy(\"UserId\").agg(avg(col(\"Rating\")).alias(\"AvgRating\"))\n",
    "sorted_user_df = user_agg.orderBy(col(\"AvgRating\").desc())\n",
    "\n",
    "sorted_user_df = sorted_user_df.select(\n",
    "    col(\"UserId\").alias(\"user\"), col(\"AvgRating\").alias(\"score\")\n",
    ")\n",
    "\n",
    "ratings_movies_df = ratings_df.join(broadcast(movies_df), on=\"MovieId\").select(\n",
    "    [\"MovieId\", \"Genres\", \"UserId\", \"Rating\"]\n",
    ")\n",
    "ratings_movies_agg = ratings_movies_df.groupBy(\"UserId\", \"Genres\").agg(\n",
    "    avg(col(\"Rating\")).alias(\"AvgRating\")\n",
    ")\n",
    "sorted_ratings_movies_df = ratings_movies_agg.orderBy(col(\"AvgRating\").desc())\n",
    "sorted_ratings_movies_df = sorted_ratings_movies_df.select(\n",
    "    col(\"UserId\").alias(\"user\"),\n",
    "    col(\"Genres\").alias(\"genre\"),\n",
    "    col(\"AvgRating\").alias(\"score\"),\n",
    ")\n",
    "\n",
    "print(\"Computing average rating score of each user for all movies...\")\n",
    "sorted_user_df.coalesce(1).write.mode(\"overwrite\").csv(\n",
    "    f\"{OUTPUT_DIR}/average_rating_by_user\", header=True, sep=\",\"\n",
    ")\n",
    "print(\"Computing average rating score of each user for all movies by genre...\")\n",
    "sorted_ratings_movies_df.coalesce(1).write.mode(\"overwrite\").csv(\n",
    "    f\"{OUTPUT_DIR}/average_rating_by_user_and_genre\", header=True, sep=\",\"\n",
    ")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf260d9",
   "metadata": {},
   "source": [
    "# Utility functions\n",
    "\n",
    "A list of utility functions to compute similarity and build utility matrix for next tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a994ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col, avg, broadcast\n",
    "\n",
    "\n",
    "def mean_normalized_ratings(for_column: str) -> DataFrame:\n",
    "    ratings_df: DataFrame = spark.read.parquet(RATINGS_FILE).select(\n",
    "        [\"UserId\", \"MovieId\", \"Rating\"]\n",
    "    )\n",
    "\n",
    "    avg_rating_df = ratings_df.groupBy(for_column).agg(\n",
    "        avg(col(\"Rating\")).alias(\"AvgRating\")\n",
    "    )\n",
    "    ratings_df = ratings_df.join(broadcast(avg_rating_df), for_column)\n",
    "    ratings_df = (\n",
    "        ratings_df.withColumn(\"NormalizedRating\", col(\"Rating\") - col(\"AvgRating\"))\n",
    "        .select([\"UserId\", \"MovieId\", \"NormalizedRating\"])\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    return ratings_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56475c3",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "\n",
    "Given any user, please list the top-’similar’ users based on the cosine similarity of previous ratings each user has given. (sorted in descending order of ‘user’ similarity score)\n",
    "\n",
    "### Output Format\n",
    "\n",
    "A list of <user, score> pairs\n",
    "sorted in descending order of ‘user’ similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19be085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Spark 3.5.0\n",
      "CPU times: user 38.6 ms, sys: 7.82 ms, total: 46.4 ms\n",
      "Wall time: 7.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.sql.functions import sqrt, pow, sum\n",
    "\n",
    "spark = spark_session()\n",
    "\n",
    "def user_similarities_ratings_based(target_user_id: str) -> DataFrame:\n",
    "    ratings_df = mean_normalized_ratings(for_column=\"UserId\")\n",
    "\n",
    "    target_user_df = ratings_df.where(f\"UserId == {target_user_id}\").select(\n",
    "        col(\"UserId\").alias(\"TargetUserId\"),\n",
    "        col(\"NormalizedRating\").alias(\"TargetUserNormalizedRating\"),\n",
    "        col(\"MovieId\"),\n",
    "    )\n",
    "    common_ratings_df = ratings_df.join(broadcast(target_user_df), on=\"MovieId\")\n",
    "\n",
    "\n",
    "    dot_df = common_ratings_df.groupBy(\"UserId\").agg(\n",
    "        sqrt(sum(pow(col(\"NormalizedRating\"), 2))).alias(\"Norm\"),\n",
    "        sum(col(\"NormalizedRating\") * col(\"TargetUserNormalizedRating\")).alias(\n",
    "            \"DotProduct\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    target_norm_val = dot_df.where(f\"UserId == {target_user_id}\").first()[\"Norm\"]\n",
    "\n",
    "    result_df = (\n",
    "        dot_df.filter(col(\"UserId\") != target_user_id)\n",
    "        .withColumn(\"Similarity\", col(\"DotProduct\") / (col(\"Norm\") * target_norm_val))\n",
    "        .select([\"UserId\", \"Similarity\"])\n",
    "    ).orderBy(col(\"Similarity\").desc())\n",
    "    return result_df\n",
    "\n",
    "## Target user to compute similarity against.\n",
    "target_user_id = \"2\"\n",
    "similarity_df = user_similarities_ratings_based(target_user_id)\n",
    "\n",
    "similarity_df.coalesce(1).write.mode(\"overwrite\").csv(\n",
    "    f\"{OUTPUT_DIR}/similar_users_to_user_{target_user_id}\", header=True, sep=\",\"\n",
    ")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fade9322",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "\n",
    "Given any movie, please list the top-’similar’ movies based on the cosine similarity of previous ratings each movie received. (sorted in descending order of ‘item’ similarity score)\n",
    "\n",
    "### Output Format\n",
    "\n",
    "a list of <movie, score> pairs\n",
    "sorted in descending order of ‘movie’ similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "130bcabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Spark 3.5.0\n",
      "CPU times: user 39.2 ms, sys: 11.4 ms, total: 50.6 ms\n",
      "Wall time: 5.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark = spark_session()\n",
    "\n",
    "def movie_similarities_ratings_based(target_movie_id: str) -> DataFrame:\n",
    "    ratings_df = mean_normalized_ratings(for_column=\"MovieId\")\n",
    "\n",
    "    target_movie_df = ratings_df.where(f\"MovieId == {target_movie_id}\").select(\n",
    "        col(\"MovieId\").alias(\"TargetMovieId\"),\n",
    "        col(\"NormalizedRating\").alias(\"TargetUserNormalizedRating\"),\n",
    "        col(\"UserId\"),\n",
    "    )\n",
    "    common_ratings_df = ratings_df.join(broadcast(target_movie_df), on=\"UserId\")\n",
    "\n",
    "\n",
    "    dot_df = common_ratings_df.groupBy(\"MovieId\").agg(\n",
    "        sqrt(sum(pow(col(\"NormalizedRating\"), 2))).alias(\"Norm\"),\n",
    "        sum(col(\"NormalizedRating\") * col(\"TargetUserNormalizedRating\")).alias(\n",
    "            \"DotProduct\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    target_norm_val = dot_df.where(f\"MovieId == {target_movie_id}\").first()[\"Norm\"]\n",
    "\n",
    "    result_df = (\n",
    "        dot_df.filter(col(\"MovieId\") != target_movie_id)\n",
    "        .withColumn(\"Similarity\", col(\"DotProduct\") / (col(\"Norm\") * target_norm_val))\n",
    "        .select([\"MovieId\", \"Similarity\"])\n",
    "    ).orderBy(col(\"Similarity\").desc())\n",
    "    return result_df\n",
    "\n",
    "## Target movie to compute similarity against.\n",
    "target_movie_id = \"2\"\n",
    "similarity_df = movie_similarities_ratings_based(target_movie_id)\n",
    "\n",
    "similarity_df.coalesce(1).write.mode(\"overwrite\").csv(\n",
    "    f\"{OUTPUT_DIR}/similar_movites_to_movie_{target_movie_id}\", header=True, sep=\",\"\n",
    ")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6a7727",
   "metadata": {},
   "source": [
    "# Task 6\n",
    "\n",
    "Implement a recommender system that recommends top-k similar movies for a given user based on collaborative filtering: item-based, and user-based. (sorted in descending order of similarity score)\n",
    "- (a) For item-based collaborative filtering: estimated by similar items\n",
    "- (b) For user-based collaborative filtering: estimated by similar users\n",
    "\n",
    "### Output format\n",
    "two lists of <movie, score> pairs: item-based, user-based\n",
    "sorted in descending order of similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a478ca87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Spark 3.5.0\n",
      "Generating User-Based Recommendations...\n",
      "Top-10 User-Based Recommendations:\n",
      "+-------+------------------+\n",
      "|MovieId|             Score|\n",
      "+-------+------------------+\n",
      "|    572|               2.0|\n",
      "|   1360|               1.5|\n",
      "|   3236|1.4999999999999998|\n",
      "|   1420| 1.267555051490446|\n",
      "|   1659|1.1856731362702553|\n",
      "|    108| 1.113359305400627|\n",
      "|   2869|1.0887100905109315|\n",
      "|   2258|               1.0|\n",
      "|   1868|               1.0|\n",
      "|    690|               1.0|\n",
      "+-------+------------------+\n",
      "\n",
      "Generating Item-Based Recommendations...\n",
      "Top-10 Item-Based Recommendations:\n",
      "+-------+-------------------+\n",
      "|MovieId|              Score|\n",
      "+-------+-------------------+\n",
      "|    889| 0.5025702416233744|\n",
      "|    404|0.40691151886043936|\n",
      "|    972| 0.3580488208820493|\n",
      "|   2258| 0.3219903536607946|\n",
      "|   2510|0.27824864917431674|\n",
      "|   1062|0.26865034769346263|\n",
      "|   1509|0.26616891733204046|\n",
      "|   2591| 0.2657844540978354|\n",
      "|     53| 0.2624776760223743|\n",
      "|    753| 0.2541647952122626|\n",
      "+-------+-------------------+\n",
      "\n",
      "CPU times: user 91.7 ms, sys: 77 ms, total: 169 ms\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.sql.functions import col, sum, sqrt, pow, abs, broadcast\n",
    "\n",
    "spark = spark_session()\n",
    "\n",
    "norm_ratings_df = mean_normalized_ratings(\"MovieId\")\n",
    "norm_ratings_df.cache()\n",
    "\n",
    "\n",
    "def recommend_user_based(target_user_id: str, k: int = 10):\n",
    "    print(\"Generating User-Based Recommendations...\")\n",
    "\n",
    "    sim_users = user_similarities_ratings_based(str(target_user_id))\n",
    "\n",
    "    candidates = norm_ratings_df.join(sim_users, on=\"UserId\")\n",
    "\n",
    "    target_user_movies = (\n",
    "        norm_ratings_df.filter(col(\"UserId\") == target_user_id)\n",
    "        .select(\"MovieId\")\n",
    "        .distinct()\n",
    "    )\n",
    "    candidates = candidates.join(target_user_movies, on=\"MovieId\", how=\"left_anti\")\n",
    "\n",
    "    recs = candidates.groupBy(\"MovieId\").agg(\n",
    "        (\n",
    "            sum(col(\"NormalizedRating\") * col(\"Similarity\"))\n",
    "            / sum(abs(col(\"Similarity\")))\n",
    "        ).alias(\"Score\")\n",
    "    )\n",
    "\n",
    "    return recs.orderBy(col(\"Score\").desc()).limit(k)\n",
    "\n",
    "\n",
    "def recommend_item_based(target_user_id: str, k: int = 10):\n",
    "    print(\"Generating Item-Based Recommendations...\")\n",
    "\n",
    "    user_movies_df = norm_ratings_df.filter(col(\"UserId\") == target_user_id).select(\n",
    "        col(\"MovieId\").alias(\"MyMovieId\"), col(\"NormalizedRating\").alias(\"MyRating\")\n",
    "    )\n",
    "\n",
    "    my_movie_ids = [row.MyMovieId for row in user_movies_df.collect()]\n",
    "\n",
    "    my_movies_ratings = norm_ratings_df.filter(\n",
    "        col(\"MovieId\").isin(my_movie_ids)\n",
    "    ).select(\n",
    "        col(\"UserId\"),\n",
    "        col(\"MovieId\").alias(\"MyMovieId\"),\n",
    "        col(\"NormalizedRating\").alias(\"MyNormRating\"),\n",
    "    )\n",
    "\n",
    "    pairs = my_movies_ratings.join(norm_ratings_df, on=\"UserId\")\n",
    "\n",
    "    movie_norms = norm_ratings_df.groupBy(\"MovieId\").agg(\n",
    "        sqrt(sum(pow(col(\"NormalizedRating\"), 2))).alias(\"Norm\")\n",
    "    )\n",
    "\n",
    "    dot_products = pairs.groupBy(\"MyMovieId\", \"MovieId\").agg(\n",
    "        sum(col(\"MyNormRating\") * col(\"NormalizedRating\")).alias(\"DotProduct\")\n",
    "    ).alias(\"dp\")\n",
    "\n",
    "    sims = (\n",
    "        dot_products.join(\n",
    "            movie_norms.alias(\"m1\"), col(\"dp.MyMovieId\") == col(\"m1.MovieId\")\n",
    "        )\n",
    "        .join(movie_norms.alias(\"m2\"), col(\"dp.MovieId\") == col(\"m2.MovieId\"))\n",
    "        .select(\n",
    "            col(\"dp.MyMovieId\"),\n",
    "            col(\"dp.MovieId\"),\n",
    "            (col(\"dp.DotProduct\") / (col(\"m1.Norm\") * col(\"m2.Norm\"))).alias(\"Similarity\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    preds = sims.join(user_movies_df, on=\"MyMovieId\")\n",
    "\n",
    "    results = preds.groupBy(\"MovieId\").agg(\n",
    "        (sum(col(\"Similarity\") * col(\"MyRating\")) / sum(abs(col(\"Similarity\")))).alias(\n",
    "            \"Score\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    results = results.filter(~col(\"MovieId\").isin(my_movie_ids))\n",
    "\n",
    "    return results.orderBy(col(\"Score\").desc()).limit(k)\n",
    "\n",
    "\n",
    "target_user_id = \"2\"\n",
    "\n",
    "ub_recs_df = recommend_user_based(target_user_id)\n",
    "print(\"Top-10 User-Based Recommendations:\")\n",
    "ub_recs_df.show()\n",
    "ub_recs_df.coalesce(1).write.mode(\"overwrite\").csv(\n",
    "    f\"{OUTPUT_DIR}/recs_user_based_user_{target_user_id}\", header=True, sep=\",\"\n",
    ")\n",
    "\n",
    "ib_recs_df = recommend_item_based(target_user_id)\n",
    "print(\"Top-10 Item-Based Recommendations:\")\n",
    "ib_recs_df.show()\n",
    "ib_recs_df.coalesce(1).write.mode(\"overwrite\").csv(\n",
    "    f\"{OUTPUT_DIR}/recs_item_based_user_{target_user_id}\", header=True, sep=\",\"\n",
    ")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf630455",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "Compute the similarities between users based on their gender, age, occupation.\n",
    "Compute the similarities between movies based on their genres."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
